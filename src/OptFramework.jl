module OptFramework

using CompilerTools

export @acc 

# This controls the debug print level.  0 prints nothing.  3 print everything.
DEBUG_LVL=0

@doc """
Control how much debugging output is generated by this module.  
Takes one Int argument where: 0 prints nothing. 
Increasing values print more debugging output up to a maximum of debug level 3.
"""
function set_debug_level(x)
    global DEBUG_LVL = x
end
 
@doc """
Calls print to print a message if the incoming debug level is greater than or equal to the level specified in set_debug_level().
First argument: the detail level of the debugging information.  Higher numbers for higher detail.
Second+ arguments: the message to print if the debug level is satisfied.
"""
function dprint(level,msgs...)
    if(DEBUG_LVL >= level)
        print(msgs...)
    end
end

@doc """
Calls println to print a message if the incoming debug level is greater than or equal to the level specified in set_debug_level().
First argument: the detail level of the debugging information.  Higher numbers for higher detail.
Second+ arguments: the message to print if the debug level is satisfied.
"""
function dprintln(level,msgs...)
    if(DEBUG_LVL >= level)
        println(msgs...)
    end
end

@doc """
Creates a typed Expr AST node.
Convenence function that takes a type as first argument and the varargs thereafter.
The varargs are used to form an Expr AST node and the type parameter is used to fill in the "typ" field of the Expr.
"""
function TypedExpr(typ, rest...)
    res = Expr(rest...)
    res.typ = typ
    res
end

@doc """
A data structure that holds information about one high-level optimization pass to run.
"func" is the callback functino that does the optimization pass and should have the signature (Expr, Tuple, Tuple)
where the Expr is the :lambda Expr for a function, the first tuple is a tuple of the types of the arguments to the
function and the second tuple is the actual name of the arguments to the functions.
"""
type optPass
    func  :: Function
    unopt :: Bool   # true if this optimization pass wants to use unoptimized code_typed form

    function optPass(f, l)
      new(f, l)
    end
end

# Stores the default set of optimization passes if they are not specified on a line-by-line basis.
optPasses = optPass[]

@doc """
Set the default set of optimization passes to apply with the @acc macro is used.
"""
function setOptPasses(passes :: Array{optPass,1} )
    unopt_first = true

    for i = 1:length(passes)
      if passes[i].unopt == true
        if unopt_first == false
          throw(string("Optimization passes cannot handle a unoptimized AST pass after an optimized typed AST pass."))
        end
      else
        unopt_first = false
      end
    end

    global optPasses = passes
end

@doc """
Get the type of an AST node.
"""
function typeOfOpr(x)
#  dprintln(3,"typeOfOpr ", x, " type = ", typeof(x))
  if isa(x, Expr) x.typ
  elseif isa(x, SymbolNode) x.typ
  elseif isa(x, GlobalRef) typeof(eval(x))
  else typeof(x) 
  end
end   

@doc """
The type for the global state of OptFramework.
It keeps track of a mapping between functions and their optimized version so that we don't try to optimize the same function with the same signature twice.
The field per_site_opt_set is used when the programmer provides the set of optimization passes along with the @acc macro.  In which case, that set of 
optimizations associated with that one @acc site is stored in per_site_opt_set.
"""
type memoizeState
  mapNameFuncInfo :: Dict{Any, Any}   # tracks the mapping from unoptimized function name to optimized function name
  per_site_opt_set

  function memoizeState()
    new(Dict{Any,Any}(), nothing)
  end
end

@doc """
The callback state variable used by create_label_map and update_labels.
label_map is a dictionary mapping old label ID's in the old AST with new label ID's in the new AST.
next_block_num is a monotonically increasing integer starting from 0 so label occur sequentially in the new AST.
last_was_label keeps track of whether we see two consecutive LabelNodes in the AST.
"""
type lmstate
  label_map
  next_block_num
  last_was_label

  function lmstate()
    new(Dict{Int64,Int64}(), 0, false)
  end
end

@doc """
An AstWalk callback that applies the label map created during create_label_map AstWalk.
For each label in the code, replace that label with the rhs of the label map.
"""
function update_labels(x, state :: lmstate, top_level_number, is_top_level, read)
  asttyp = typeof(x)
  if asttyp == LabelNode
    return [LabelNode(state.label_map[x.label])]
  elseif asttyp == GotoNode
    return [GotoNode(state.label_map[x.label])]
  elseif asttyp == Expr
    head = x.head
    args = x.args
    if head == :gotoifnot
      else_label = args[2]
      x.args[2] = state.label_map[else_label]
      return [x]
    end
  end
  return nothing
end

@doc """
An AstWalk callback that collects information about labels in an AST.
The labels in AST are generally not sequential but to feed back into a Function Expr
correctly they need to be.  So, we keep a map from the old label in the AST to a new label
that we monotonically increases.
If we have code in the AST like the following:
   1:
   2:
... then one of these labels is redundant.  We set "last_was_label" if the last AST node
we saw was a label.  If we see another LabelNode right after that then we duplicate the rhs
of the label map.  For example, if you had the code:
   5:
   4:
... and the label 5 was the third label in the code then in the label map you would then have:
   5 -> 3, 4 -> 3.
This indicates that uses of both label 5 and label 4 in the code will become label 3 in the modified AST.
"""
function create_label_map(x, state :: lmstate, top_level_number, is_top_level, read)
  asttyp = typeof(x)
  if asttyp == LabelNode
    if state.last_was_label
      state.label_map[x.label] = state.next_block_num-1
    else
      state.label_map[x.label] = state.next_block_num
      state.next_block_num += 1
    end
    state.last_was_label = true
  else
    state.last_was_label = false
  end
  return nothing
end

@doc """
Sometimes update_labels creates two label nodes that are the same.
This function removes such duplicate labels.
"""
function removeDupLabels(stmts)
  if length(stmts) == 0
    return Any[]
  end

  ret = Any[]

  push!(ret, stmts[1])

  for i = 2:length(stmts)
    if !(typeof(stmts[i]) == LabelNode && typeof(stmts[i-1]) == LabelNode)
      push!(ret, stmts[i])
    end
  end

  ret
end

@doc """
Makes sure that a newly created function is correctly present in the internal Julia method table.
"""
function tfuncPresent(func, tt)
  m = methods(func, tt)[1]
  def = m.func.code
  if def.tfunc == ()
    dprintln(1, "tfunc NOT present before code_typed")
    code_typed(func, tt)
    if def.tfunc == ()
      dprintln(1, "tfunc NOT present after code_typed")
    else
      dprintln(1, "tfunc present after code_typed")
    end
  else
    dprintln(1, "tfunc present on call")
  end 
end

@doc """
Converts the unoptimized typed AST for a function/signature to the optimized typed AST version.
"""
function unoptToTyped(func :: Function, unoptAst, call_sig_arg_tuple)
  new_func_name = string(string(func), "_unoptToTyped")  # A temporary function to perform this conversion.
  nfsym = symbol(new_func_name)                          # The symbol for the temporary function.
  assert(typeof(unoptAst) == Expr && unoptAst.head == :lambda)

  # The basic approach is to put the unoptimized AST into a temporary function and then call optimized code_typed to
  # get the corresponding optimized version of the AST.
  #
  # This process has the same problem with basic block labelling as we do in copyFunctionNewName and so the same
  # process is employed to relabel the function into a form that Julia will accept.
  copy_args = unoptAst.args[1]
  body = unoptAst.args[3]
  assert(typeof(body) == Expr && body.head == :body)
  dprintln(2,"unoptToTyped typeof(copy_args) = ", typeof(copy_args), " ", copy_args)
  dprintln(2,"unoptToTyped typeof(unoptAst.args[3]) = ", typeof(unoptAst.args[3]), " ", unoptAst.args[3])
  state = lmstate()
  CompilerTools.AstWalker.AstWalk(body, create_label_map, state)
  #dprintln(3,"label mapping = ", state.label_map)
  state.last_was_label = false
  unoptAst.args[3] = CompilerTools.AstWalker.get_one(CompilerTools.AstWalker.AstWalk(unoptAst.args[3], update_labels, state))
  unoptAst.args[3].args = removeDupLabels(unoptAst.args[3].args)
  new_func = Expr(:function, Expr(:call, nfsym, copy_args...), Expr(:block, unoptAst.args[3].args...))
  dprintln(2,"unoptToTyped new body = ", unoptAst.args[3])
  # Evaluate the function expression to force the temporary function into existence.  
  # The function is eval'ed into existence in the same module in which the original function existed.
  eval_new_func = Base.function_module(func, call_sig_arg_tuple).eval(new_func)
  if DEBUG_LVL >= 3
    lambda = code_lowered(eval_new_func, call_sig_arg_tuple)[1]
    println("lowered copy = \n", lambda)
  end
  # Now just return the optimized AST from the newly created temporary function.
  return code_typed(eval_new_func, call_sig_arg_tuple)[1]
end

@doc """
Create a copy of a function.
1) old_func - the function to copy.
2) new_func_name - the name of the new copy of function.
3) arg_tuple - the signature of the function
"""
function copyFunctionNewName(old_func, new_func_name :: AbstractString, arg_tuple)
  lambda = code_lowered(old_func, arg_tuple)[1] # Get the code_lowered AST form for the function to be copied.
  nfsym  = symbol(new_func_name)                 # Create a symbol for the new function name.
  dprintln(3, "copying old_func = \n", lambda)

  dprintln(3, "lambda = ", lambda)
  # You can't just take the output of code_lowered and put that in a new function and have it work.
  # It will complain about certain facets of the basic block labelling and CFG structure.  Here
  # we re-number and simplify basic blocks numbering to a form that is acceptable.
  copy_args = lambda.args[1]
  copy_body = lambda.args[3].args
  dprintln(3,"copyFunctionNewName body = \n", lambda.args[3], " length = ", length(copy_body))
  state = lmstate()
  CompilerTools.AstWalker.AstWalk(lambda.args[3], create_label_map, state)
  #dprintln(3,"label mapping = ", state.label_map)
  state.last_was_label = false
  lambda.args[3] = CompilerTools.AstWalker.get_one(CompilerTools.AstWalker.AstWalk(lambda.args[3], update_labels, state))
  copy_body = lambda.args[3].args = removeDupLabels(lambda.args[3].args)
  #dprintln(3,"after label renumberingl = \n", lambda.args[3], " type = ", typeof(lambda.args[3]), " copy_body = ", copy_body, " type = ", typeof(copy_body))
 
  # Create a new function whose name is in nfsym and whose arguments are in copy_args and whose body is copy_body.
  new_func = Expr(:function, Expr(:call, nfsym, copy_args...), Expr(:block, copy_body...))
  # Evaluate the function expression to force the function into existence.  
  # The function is eval'ed into existence in the same module in which the original function existed.
  old_func_mod = Base.function_module(old_func, arg_tuple)
  dprintln(3, "old_func = ", old_func, " arg_tuple = ", arg_tuple, " old_func_mod = ", old_func_mod, " old_func_mod type = ", typeof(old_func_mod))
  dprintln(3, "new_func = ", new_func)
  eval_new_func = old_func_mod.eval(new_func)
  if DEBUG_LVL >= 3
    lambda = code_lowered(eval_new_func, arg_tuple)[1]
    dprintln(3, "new copied func = \n", lambda)
    tfuncPresent(eval_new_func, arg_tuple)
  end
  code_typed(eval_new_func, arg_tuple) # force tfunc to be created in methods[1].func.code
  return eval_new_func
end

@doc """
Takes a function, a signature, argument names, and a set of optimizations and applies that set of optimizations to the function,
creating a new optimized function.  Argument explanation follows:
1) func_expr - the function being optimized
2) call_sig_arg_tuple - the signature of the function, i.e., the types of each of its arguments
3) call_sig_args - the actual argument names of the function
4) per_site_opt_set - the set of optimization passes to apply to this function.
"""
function processFuncCall(func_expr, call_sig_arg_tuple, call_sig_args, per_site_opt_set)
  fetyp = typeof(func_expr)
  dprintln(3,"processFuncCall ", func_expr, " module = ", Base.function_module(func_expr, call_sig_arg_tuple), " ", call_sig_arg_tuple, " ", fetyp, " args = ", call_sig_args, " opt_set = ", per_site_opt_set)
  func = eval(func_expr)
  dprintln(3,"func = ", func, " type = ", typeof(func))

  ftyp = typeof(func)
  dprintln(4,"After name resolution: func = ", func, " type = ", ftyp)
  if ftyp == DataType
    return nothing
  end
  assert(ftyp == Function || ftyp == IntrinsicFunction || ftyp == LambdaStaticData)

  # If per site opt set has not been provided then use the global default.
  if per_site_opt_set == nothing
    per_site_opt_set = optPasses
  else
    assert(isa(per_site_opt_set, Array))
    for s in per_site_opt_set
      assert(typeof(s) == optPass)
    end
  end

  # We can only optimize Functions, not IntrinsicFunction or LambdaStaticData.
  if ftyp == Function
    if length(per_site_opt_set) == 0
      throw(string("There are no registered optimization passes."))
    end

    # Create a copy of the incoming function.  We will then optimize the copy so the original unoptimized function can still be called.
    new_func_name = string(string(func),"_processFuncCall")
    new_func = copyFunctionNewName(func, new_func_name, call_sig_arg_tuple)
    dprintln(2,"temp_func is ", new_func)

    # Some optimization passes want to work on unoptmized code_typed AST whereas others want to work on optimized (i.e., regular) code-typed AST.
    # All unoptimized passes have to preceed any optimized pass because we can't convert from modified optimized AST back to modified unoptimized AST.
    # Here we see which kind of AST we will start with.
    last_unopt = per_site_opt_set[1].unopt

    # Get the AST on which the first optimization pass wants to work.
    if last_unopt == true
      cur_ast = code_typed(new_func, call_sig_arg_tuple, optimize=false)[1]   # false means generate type information but don't otherwise optimize
    else
      cur_ast = code_typed(new_func, call_sig_arg_tuple)[1]
    end
    assert(typeof(cur_ast) == Expr)
    assert(cur_ast.head == :lambda)

    dprintln(3,"Initial code to optimize = ", cur_ast)

    # For each optimization pass in the optimization set.
    for i = 1:length(per_site_opt_set)
      # See if the current optimization pass uses optimized AST form and the previous optimization pass used unoptimized AST form.
      if per_site_opt_set[i].unopt != last_unopt
        # If so, convert unoptimized AST to optimized AST form.
        cur_ast = unoptToTyped(new_func, cur_ast, call_sig_arg_tuple)
      end
      last_unopt = per_site_opt_set[i].unopt
      assert(typeof(cur_ast) == Expr && cur_ast.head == :lambda)
      assert(typeof(cur_ast.args[3]) == Expr && cur_ast.args[3].head == :body)

      # Call the current optimization on the current AST.
      cur_ast = per_site_opt_set[i].func(cur_ast, call_sig_arg_tuple, call_sig_args)

      assert(typeof(cur_ast) == Expr && cur_ast.head == :lambda)
      assert(typeof(cur_ast.args[3]) == Expr && cur_ast.args[3].head == :body)

      if typeof(cur_ast) == Function
        dprintln(3,"Optimization pass returned a function.")
        if i != length(per_site_opt_set)
          dprintln(0,"A non-final optimization pass returned a Function so later optimization passes will not run.")
        end
        return cur_ast
      else
        dprintln(3,"AST after optimization pass ", i, " = ", cur_ast)
        if typeof(cur_ast) != Expr
          dprintln(0, "cur_ast after opt pass not an expression. type = ", typeof(cur_ast))
        end
      end
    end

    # If the last optimization pass operated on unoptimized AST, then we need to convert to optimized AST before we can store the AST
    # back into the optimized copy of the function.
    if last_unopt == true
      assert(typeof(cur_ast) == Expr && cur_ast.head == :lambda)
      assert(typeof(cur_ast.args[3]) == Expr && cur_ast.args[3].head == :body)
      body = cur_ast.args[3]
      dprintln(3,"Last opt pass worked on unopt.\n", body)

      cur_ast = unoptToTyped(new_func, cur_ast, call_sig_arg_tuple)

      assert(typeof(cur_ast) == Expr && cur_ast.head == :lambda)
      assert(typeof(cur_ast.args[3]) == Expr && cur_ast.args[3].head == :body)

      body = cur_ast.args[3]
      dprintln(3,"Last opt pass after converting to typed AST.\n", body)
    end

    # Write the modifed code back to the function.
    dprintln(2,"Before methods at end of processFuncCall.")
    tfuncPresent(new_func, call_sig_arg_tuple)
    method = methods(new_func, call_sig_arg_tuple)
    assert(length(method) == 1)
    method = method[1]
    method.func.code.tfunc[2] = ccall(:jl_compress_ast, Any, (Any,Any), method.func.code, cur_ast)

    dprintln(3,"Final processFuncCall = ", code_typed(new_func, call_sig_arg_tuple)[1])
    return new_func
  end
  return nothing
end

@doc """
The global state of OptFramework.
Memoizes the optimization of function/signature pairs.
Holds per call site optimization set for the second flavor of the @acc macro.
"""
gOptFrameworkState = memoizeState()

@doc """
An AstWalk callback function.
Finds call sites in the AST and replaces them with calls to newly generated trampoline functions.
These trampolines functions allow us to capture runtime types which in turn enables optimization passes to run on fully typed AST.
If a function/signature combination has not previously been optimized then call processFuncCall to optimize it.
"""
function opt_calls_insert_trampoline(x, state :: memoizeState, top_level_number, is_top_level, read)
  if typeof(x) == Expr
    if x.head == :call
      # We found a call expression within the larger expression.
      call_expr = x.args[1]          # Get the target of the call.
      call_sig_args = x.args[2:end]  # Get the arguments to the call.
      dprintln(2, "Start opt_calls = ", call_expr, " signature = ", call_sig_args, " typeof(call_expr) = ", typeof(call_expr))

      # The name of the new trampoline function.
      new_func_name = string("opt_calls_trampoline_", string(call_expr))
      new_func_sym  = symbol(new_func_name)

      # Recursively process the arguments to this function possibly finding other calls to replace.
      for i = 2:length(x.args)
        new_arg = CompilerTools.AstWalker.AstWalk(x.args[i], opt_calls_insert_trampoline, state)
        assert(isa(new_arg,Array))
        assert(length(new_arg) == 1)
        x.args[i] = new_arg[1]
      end

      # Form a tuple of the function name and arguments.
      dprintln(3,"Creating new trampoline for ", call_expr)
      dprintln(2, new_func_sym)
      for i = 1:length(call_sig_args)
        dprintln(2, "    ", call_sig_args[i])
      end
      per_site_opt_set = state.per_site_opt_set 
 
      # Define the trampoline.
      @eval function ($new_func_sym)(orig_func, per_site_opt_set, $(call_sig_args...))
              # Create a tuple of the actual argument types for this invocation.
              call_sig = Expr(:tuple)
              call_sig.args = map(typeOfOpr, Any[ $(call_sig_args...) ]) 
              call_sig_arg_tuple = eval(call_sig)
              #println(call_sig_arg_tuple)

              # Create a tuple of function and argument types.
              fs = ($new_func_sym, call_sig_arg_tuple, per_site_opt_set)
              dprintln(1, "running ", $new_func_name, " fs = ", fs)

              # If we have previously optimized this function and type combination ...
              if haskey(gOptFrameworkState.mapNameFuncInfo, fs)
                # ... then call the function we previously optimized.
                func_to_call = gOptFrameworkState.mapNameFuncInfo[fs]
              else
                # ... else see if we can optimize it.
                process_res = processFuncCall(orig_func, call_sig_arg_tuple, $call_sig_args, per_site_opt_set)

                if process_res != nothing
                  # We did optimize it in some way we will call the optimized version.
                  dprintln(3,"processFuncCall DID optimize ", orig_func)
                  func_to_call = process_res
                else
                  # We did not optimize it so we will call the original function.
                  dprintln(3,"processFuncCall didn't optimize ", orig_func)
                  func_to_call = orig_func
                end
                # Remember this optimization result for this function/type combination.
                gOptFrameworkState.mapNameFuncInfo[fs] = func_to_call
              end

              dprintln(1, "Executing function = ", Base.function_name(func_to_call), " module = ", Base.function_module(func_to_call, call_sig_arg_tuple))
              #dprintln(3,"Code to call = ", code_typed(func_to_call, call_sig_arg_tuple)[1])
              # Call the function.
              ret = func_to_call($(call_sig_args...))
              #dprintln(3,"Code to call after = ", code_typed(func_to_call, call_sig_arg_tuple)[1])
              ret
            end

      # Update the call expression to call our trampoline and pass the original function so that we can
      # call it if nothing can be optimized.
      resolved_name = @eval OptFramework.$new_func_sym
      x.args = [ resolved_name; call_expr; per_site_opt_set; x.args[2:end] ]

      dprintln(2, "Replaced call_expr = ", call_expr, " type = ", typeof(call_expr), " new = ", x.args[1])

      return [x]
    end    
  end
  nothing
end

@doc """
The formation of the code to be returned from the @acc macro is handled here.
We use AstWalk to search for callsites via the opt_calls_insert_trampoline callback and to then insert trampolines.
That updated expression containing trampoline calls is then returned as the generated code from the @acc macro.
"""
function convert_expr(ast)
  dprintln(2, "convert_expr ", ast, " ", typeof(ast), " gOptFrameworkState = ", gOptFrameworkState)
  res = CompilerTools.AstWalker.AstWalk(ast, opt_calls_insert_trampoline, gOptFrameworkState)
  assert(isa(res,Array))
  assert(length(res) == 1)
  dprintln(2,"converted expression = ", res[1])
  return esc(res[1])
end

@doc """
The @acc macro comes in two forms:
1) @acc expression
2) @acc Array{optPass,1} expression.
In the first form, the set of optimization passes to apply come from the default set of optimization passes as specified with the funciton setOptPasses.
In the second form, an array of optPass's comes right after @acc and this is followed by an expression.  In this case, the specified set of
   optPasses are used just for optimizing the following expression.
The @acc macro replaces each call in the expression with a call to a trampolines that determines the types of the call and if that combination of function and signature
has not previously been optimized then it calls the set of optimization passes to optimize it.  Then, the trampoline calls the optimized function.
"""
macro acc(ast1, ast2...)
  if isempty(ast2)
#    println("old acc code")
    gOptFrameworkState.per_site_opt_set = nothing
    return convert_expr(ast1)
  else
#    println("new acc code")
    gOptFrameworkState.per_site_opt_set = ast1
    return convert_expr(ast2[1]) 
  end
end

end   # end of module
